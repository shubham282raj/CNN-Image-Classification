{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_url = \"./CUB_200_2011/images.txt\"\n",
    "train_test_split_url = \"./CUB_200_2011/train_test_split.txt\"\n",
    "classes_url = \"./CUB_200_2011/classes.txt\"\n",
    "image_class_labels_url = \"./CUB_200_2011/image_class_labels.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.genfromtxt(images_url , delimiter=' ', dtype=str) #<image_id> <image_name>\n",
    "train_test_split = np.genfromtxt(train_test_split_url, delimiter=\" \", dtype=str) #<image_id> <is_training_image>\n",
    "classes = np.genfromtxt(classes_url, delimiter=\" \", dtype=str) #<class_id> <class_name>\n",
    "image_class_labels = np.genfromtxt(image_class_labels_url, delimiter=\" \", dtype=str).astype(int) #<image_id> <class_id>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>image</td>\n",
    "        <td>&lt;image_id&gt; &lt;image_name&gt;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>train_test_split</td>\n",
    "        <td>&lt;image_id&gt; &lt;is_training_image&gt;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>classes</td>\n",
    "        <td>&lt;class_id&gt; &lt;class_name&gt;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>image_class_labels</td>\n",
    "        <td>&lt;image_id&gt; &lt;class_id&gt;</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = {int(row[0]): \"CUB_200_2011/images/\"+row[1] for row in image}\n",
    "train_test_split = {int(row[0]): int(row[1]) for row in train_test_split}\n",
    "classes = {int(row[0]): row[1] for row in classes}\n",
    "image_class_labels = {int(row[0]): int(row[1]) for row in image_class_labels}\n",
    "print(len(image),len(train_test_split),len(classes),len(image_class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_name = 500\n",
    "\n",
    "print(image_name, image[image_name])\n",
    "print(image_name, image_class_labels[image_name])\n",
    "print(image_name, classes[image_class_labels[image_name]])\n",
    "print(image_name, train_test_split[image_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = np.array([key for key, value in train_test_split.items() if value == 1])\n",
    "image_test = np.array([key for key, value in train_test_split.items() if value == 0])\n",
    "\n",
    "image_split = {\n",
    "    \"train\": image_train,\n",
    "    \"test\": image_test\n",
    "}\n",
    "\n",
    "n_train = len(image_train)\n",
    "n_test = len(image_test)\n",
    "n_classes = len(classes)\n",
    "\n",
    "print(\"Number of Images:\", len(image))\n",
    "print(f\"n_train: {n_train}\")\n",
    "print(f\"n_test: {n_test}\")\n",
    "print(f\"n_classes: {n_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>image_train</td>\n",
    "        <td>list of &lt;image_id&gt;</td>\n",
    "        <td>&lt;is_training_image&gt; == 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>image_test</td>\n",
    "        <td>list of &lt;image_id&gt;</td>\n",
    "        <td>&lt;is_training_image&gt; == 0</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(image):\n",
    "    plt.imshow(plt.imread(image))\n",
    "    plt.axis(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showImage(image[440])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### EfficientNet_B1_V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_out_ftrs = 200 # number of classes for out classification is 200\n",
    "image_input_size = 244 # 244x244 image; imagas are resized to this size\n",
    "batch_size = 4\n",
    "# batch_size_test = 1\n",
    "learning_rate = 0.1\n",
    "num_epoch = 10\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "\n",
    "# Load pre-trained MobileNetV3 model\n",
    "model = mobilenet_v3_large(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in original model: {total_params}\")\n",
    "\n",
    "num_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Total number of training parameters in original model: {num_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all layers for fine tuning (not doing this takes it very long to train)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "print(model.classifier[3].in_features)\n",
    "#number of inputs in last layer\n",
    "num_ftrs = model.classifier[3].in_features\n",
    "new_layers = [\n",
    "    nn.Linear(num_ftrs, 200)\n",
    "]\n",
    "\n",
    "model.classifier[3] = nn.Sequential(*new_layers)\n",
    "\n",
    "\n",
    "# printing the last layer : classifier\n",
    "print(model.classifier)\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters in fine-tuned model: {total_params}\")\n",
    "\n",
    "num_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Total number of training parameters in fine-tuned model: {num_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset in torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandGrayscales:\n",
    "    def __call__(self, sample):\n",
    "        return sample.expand(3,-1,-1)\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((image_input_size,image_input_size)),\n",
    "        transforms.ToTensor(),\n",
    "        ExpandGrayscales()\n",
    "    ]\n",
    ")\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.transforms.transforms import RandomPerspective, RandomAffine\n",
    "\n",
    "augmentation_trasform = transforms.Compose([\n",
    "    transforms.Resize((image_input_size, image_input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.5,\n",
    "        contrast=0.2,\n",
    "        saturation=0.2,\n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    RandomPerspective(distortion_scale=0.5, p=0.5),\n",
    "    RandomAffine(degrees=45, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=15),\n",
    "    transforms.RandomGrayscale(p=0.05),\n",
    "    transforms.ToTensor(),\n",
    "    ExpandGrayscales()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, image_id, id_to_url, image_class_labels, transform):\n",
    "        self.x = [id_to_url[x] for x in image_id]\n",
    "        self.y = [image_class_labels[x] for x in image_id]\n",
    "        self.n_samples = len(image_id)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.x[index])\n",
    "        img = self.transform(img)\n",
    "        return img, self.y[index]-1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_train_dataset = CUBDataset(\n",
    "    image_id=image_split[\"train\"],\n",
    "    id_to_url=image,\n",
    "    image_class_labels=image_class_labels,\n",
    "    transform=augmentation_trasform\n",
    ")\n",
    "\n",
    "augment_batch_size = 20\n",
    "\n",
    "augment_train_dataloader = DataLoader(\n",
    "    dataset=augment_train_dataset,\n",
    "    batch_size=augment_batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_num = int(time.time())\n",
    "folder_path = \"CUB_200_2011/aug_images\"\n",
    "\n",
    "if not os.path.exists(folder_path):\n",
    "\n",
    "    os.makedirs(folder_path)\n",
    "    for _ in range(3):\n",
    "        for images, labels in augment_train_dataloader:\n",
    "            for i in range(images.shape[0]):\n",
    "                file_path = folder_path + \"/\" + str(int(labels[i])+1) + \"_img\" + str(img_num) + \".png\"\n",
    "                save_image(images[i], file_path)\n",
    "                image[img_num] = file_path\n",
    "                image_split[\"train\"] = np.append(image_split[\"train\"], img_num)\n",
    "                image_class_labels[img_num] = (int(labels[i])+1)\n",
    "                img_num += 1\n",
    "\n",
    "else:\n",
    "    file_names = os.listdir(folder_path)\n",
    "    for file_name in file_names:\n",
    "        class_id = int(file_name.split(\"_\")[0])\n",
    "        file_path = folder_path + \"/\" + file_name\n",
    "        image[img_num] = file_path\n",
    "        image_split[\"train\"] = np.append(image_split[\"train\"], img_num)\n",
    "        image_class_labels[img_num] = (class_id)\n",
    "        img_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.shuffle(image_split[\"train\"])\n",
    "np.random.shuffle(image_split[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(image), len(image_split[\"train\"]), len(image_split[\"test\"]), len(image_class_labels))\n",
    "print(f\"Train + Test: {len(image)}\")\n",
    "print(f\"Train: {len(image_split['train'])}\")\n",
    "print(f\"Test: {len(image_split['test'])}\")\n",
    "print(f\"Image Class Label Length: {len(image_class_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Actual Train / Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    x: CUBDataset(\n",
    "        image_id=image_split[x],\n",
    "        id_to_url=image, \n",
    "        image_class_labels=image_class_labels,\n",
    "        transform=transform\n",
    "    )\n",
    "    for x in [\"train\", \"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    x: DataLoader(\n",
    "        dataset=datasets[x],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    for x in [\"train\", \"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {\n",
    "    x: len(datasets[x])\n",
    "    for x in [\"train\", \"test\"]\n",
    "}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_sizes = {\n",
    "    x: len(dataloaders[x])\n",
    "    for x in [\"train\", \"test\"]\n",
    "}\n",
    "dataloader_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    lr=learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_lr_schedular = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer=optimizer,\n",
    "    step_size=7,\n",
    "    gamma=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[151], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     15\u001b[0m running_corrects \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mp:\\Projects\\libs\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mp:\\Projects\\libs\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mp:\\Projects\\libs\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mp:\\Projects\\libs\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[140], line 10\u001b[0m, in \u001b[0;36mCUBDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m      9\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx[index])\n\u001b[1;32m---> 10\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my[index]\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mp:\\Projects\\libs\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mp:\\Projects\\libs\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mp:\\Projects\\libs\\Lib\\site-packages\\torchvision\\transforms\\functional.py:173\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    171\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_total_steps = dataloader_sizes[\"train\"]\n",
    "\n",
    "print(f\"Training Started on {device}\")\n",
    "train_time = 0\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epoch}\" + \"-\" * 20)\n",
    "\n",
    "    #each epoch has a training and a validation phase\n",
    "    for phase in [\"train\", \"test\"]:\n",
    "        time_start = time.time()\n",
    "        model.train() if phase == \"train\" else model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0.0\n",
    "\n",
    "        for i, (images, labels) in enumerate(dataloaders[phase]):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                outputs = model(images)\n",
    "                output_one_hot = torch.argmax(outputs, dim=1)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # backward + optimize only if in training loop\n",
    "                if phase == \"train\":\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # with torch.no_grad():\n",
    "            #     output_one_hot = outputs.argmax(dim=1)\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            running_corrects += torch.sum(output_one_hot == labels.data)\n",
    "\n",
    "        if phase == \"train\":\n",
    "                step_lr_schedular.step()\n",
    "\n",
    "        epoch_loss = running_loss / dataset_sizes[phase]\n",
    "        epoch_acc = 100 * running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "        \n",
    "        epoch_time = time.time() - time_start\n",
    "        if phase == \"train\":\n",
    "            train_time += epoch_time\n",
    "\n",
    "        print(f\"{phase} Loss: {epoch_loss:.2f} Acc: {epoch_acc:.2f}\", f\"Time_Taken: {epoch_time}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "print(f\"Training Finished in {train_time//60:.0f}m {train_time%60:.0f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = \"num_epoch_2_all_grad_on.pth\"\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
