{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_url = \"./CUB_200_2011/images.txt\"\n",
    "train_test_split_url = \"./CUB_200_2011/train_test_split.txt\"\n",
    "classes_url = \"./CUB_200_2011/classes.txt\"\n",
    "image_class_labels_url = \"./CUB_200_2011/image_class_labels.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.genfromtxt(images_url , delimiter=' ', dtype=str) #<image_id> <image_name>\n",
    "\n",
    "train_test_split = np.genfromtxt(train_test_split_url, delimiter=\" \", dtype=str) #<image_id> <is_training_image>\n",
    "\n",
    "classes = np.genfromtxt(classes_url, delimiter=\" \", dtype=str) #<class_id> <class_name>\n",
    "\n",
    "image_class_labels = np.genfromtxt(image_class_labels_url, delimiter=\" \", dtype=str).astype(int) #<image_id> <class_id>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>image</td>\n",
    "        <td>&lt;image_id&gt; &lt;image_name&gt;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>train_test_split</td>\n",
    "        <td>&lt;image_id&gt; &lt;is_training_image&gt;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>classes</td>\n",
    "        <td>&lt;class_id&gt; &lt;class_name&gt;</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>image_class_labels</td>\n",
    "        <td>&lt;image_id&gt; &lt;class_id&gt;</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = {int(row[0]): \"CUB_200_2011/images/\"+row[1] for row in image}\n",
    "\n",
    "train_test_split = {int(row[0]): int(row[1]) for row in train_test_split}\n",
    "\n",
    "classes = {int(row[0]): row[1] for row in classes}\n",
    "\n",
    "image_class_labels = {int(row[0]): int(row[1]) for row in image_class_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = 5\n",
    "# print(row, image[row])\n",
    "# print(row, train_test_split[row])\n",
    "# print(row, classes[row])\n",
    "# print(row, image_class_labels[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_train = {key: value for key, value in  train_test_split.items() if value == 1}\n",
    "# image_test = {key: value for key, value in  train_test_split.items() if value == 0}\n",
    "image_train = np.array([key for key, value in train_test_split.items() if value == 1])\n",
    "image_test = np.array([key for key, value in train_test_split.items() if value == 0])\n",
    "\n",
    "image_split = {\n",
    "    \"train\": image_train,\n",
    "    \"test\": image_test\n",
    "}\n",
    "\n",
    "n_train = len(image_train)\n",
    "n_test = len(image_test)\n",
    "n_classes = len(classes)\n",
    "\n",
    "print(\"Number of images:\", len(image))\n",
    "print(f\"Number of training images by default: {n_train}\")\n",
    "print(f\"Number of testing images by default: {n_test}\")\n",
    "print(f\"Number of classes: {n_classes}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>image_train</td>\n",
    "        <td>list of &lt;image_id&gt;</td>\n",
    "        <td>&lt;is_training_image&gt; == 1</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>image_test</td>\n",
    "        <td>list of &lt;image_id&gt;</td>\n",
    "        <td>&lt;is_training_image&gt; == 0</td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImage(image):\n",
    "    plt.imshow(plt.imread(image))\n",
    "    plt.axis(False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_out_ftrs = 200 # number of classes for out classification is 200\n",
    "image_input_size = 244 # 244x244 image; imagas are resized to this size\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_params(model):  \n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total number of parameters in model: {total_params}\")\n",
    "\n",
    "    training_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "    print(f\"Total number of training parameters in model: {training_params}\")\n",
    "\n",
    "    return total_params, training_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset in torch dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpandGrayscales:\n",
    "    def __call__(self, sample):\n",
    "        return sample.expand(3,-1,-1)\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomResizedCrop((image_input_size, image_input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    ExpandGrayscales()\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.CenterCrop((image_input_size, image_input_size)),\n",
    "    transforms.ToTensor(),\n",
    "    ExpandGrayscales()\n",
    "])\n",
    "\n",
    "trasform_augmentation = transforms.Compose([\n",
    "    transforms.Resize((300,300)),\n",
    "    transforms.ColorJitter(\n",
    "        brightness=0.5,\n",
    "        contrast=0.3,\n",
    "        saturation=0.3,\n",
    "        hue=0.05\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.CenterCrop((250,250)),\n",
    "    transforms.ToTensor(),\n",
    "    ExpandGrayscales()\n",
    "])\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": transform_train,\n",
    "    \"test\": transform_test,\n",
    "    \"augmentation\": trasform_augmentation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, image_id, id_to_url, image_class_labels, transform):\n",
    "        self.x = [id_to_url[x] for x in image_id]\n",
    "        self.y = [image_class_labels[x] for x in image_id]\n",
    "        self.n_samples = len(image_id)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.x[index])\n",
    "        img = self.transform(img)\n",
    "        return img, self.y[index]-1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_train_dataset = CUBDataset(\n",
    "    image_id=image_split[\"train\"],\n",
    "    id_to_url=image,\n",
    "    image_class_labels=image_class_labels,\n",
    "    transform=data_transforms[\"augmentation\"]\n",
    ")\n",
    "\n",
    "augment_batch_size = 20\n",
    "\n",
    "augment_train_dataloader = DataLoader(\n",
    "    dataset=augment_train_dataset,\n",
    "    batch_size=augment_batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation():\n",
    "    img_num = int(time.time())\n",
    "    folder_path = \"CUB_200_2011/aug_images\"\n",
    "\n",
    "    if not os.path.exists(folder_path):\n",
    "        print(\"Data Augmentation in process\")\n",
    "        print(\"This might take few minutes\\nPlease wait...\")\n",
    "        os.makedirs(folder_path)\n",
    "        for _ in range(3):\n",
    "            for images, labels in augment_train_dataloader:\n",
    "                for i in range(images.shape[0]):\n",
    "                    file_path = folder_path + \"/\" + str(int(labels[i])+1) + \"_img\" + str(img_num) + \".png\"\n",
    "                    save_image(images[i], file_path)\n",
    "                    image[img_num] = file_path\n",
    "                    image_split[\"train\"] = np.append(image_split[\"train\"], img_num)\n",
    "                    image_class_labels[img_num] = (int(labels[i])+1)\n",
    "                    img_num += 1\n",
    "        print(\"Data Augmentation Completed\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"Loading Augmented Images\")\n",
    "        file_names = os.listdir(folder_path)\n",
    "        for file_name in file_names:\n",
    "            class_id = int(file_name.split(\"_\")[0])\n",
    "            file_path = folder_path + \"/\" + file_name\n",
    "            image[img_num] = file_path\n",
    "            image_split[\"train\"] = np.append(image_split[\"train\"], img_num)\n",
    "            image_class_labels[img_num] = (class_id)\n",
    "            img_num += 1\n",
    "        print(\"Augmented Images Loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "np.random.shuffle(image_split[\"train\"])\n",
    "np.random.shuffle(image_split[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of images after augmentation: {len(image)}\")\n",
    "print(f\"Number of training images after augmentation: {len(image_split[\"train\"])}\")\n",
    "print(f\"Number of testing images: {len(image_split[\"test\"])}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Actual Train / Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    x: CUBDataset(\n",
    "        image_id=image_split[x],\n",
    "        id_to_url=image, \n",
    "        image_class_labels=image_class_labels,\n",
    "        transform=data_transforms[x]\n",
    "    )\n",
    "    for x in [\"train\", \"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {\n",
    "    x: DataLoader(\n",
    "        dataset=datasets[x],\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "    for x in [\"train\", \"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes = {\n",
    "    x: len(datasets[x])\n",
    "    for x in [\"train\", \"test\"]\n",
    "}\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_sizes = {\n",
    "    x: len(dataloaders[x])\n",
    "    for x in [\"train\", \"test\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc_loss(losses, accuracies):\n",
    "    num_epoch = len(losses['train'])\n",
    "    epochs = [x+1 for x in range(num_epoch)]\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(epochs, losses[\"train\"], label=\"Training Loss\", marker='o')\n",
    "    plt.plot(epochs, losses[\"test\"], label=\"Testing Loss\", marker='o')\n",
    "    plt.title('Training and Testing Losses')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(epochs, accuracies[\"train\"], label=\"Training Accuracy\", marker='o')\n",
    "    plt.plot(epochs, accuracies[\"test\"], label=\"Testing Accuracy\", marker='o')\n",
    "    plt.title('Training and Testing Accuracies')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "                criterion = nn.CrossEntropyLoss(),\n",
    "                learning_rate=learning_rate,\n",
    "                optimizer = None, \n",
    "                schedular=None, \n",
    "                num_epoch=num_epoch,\n",
    "                save_checkpoint=False):\n",
    "    \n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.Adam(\n",
    "            params=model.parameters(),\n",
    "            lr=learning_rate\n",
    "        )\n",
    "\n",
    "    print(f\"Training Started on {device}\")\n",
    "    train_time = 0\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    epoch_loss_list = {\n",
    "        \"train\": [],\n",
    "        \"test\": []\n",
    "    }\n",
    "\n",
    "    epoch_acc_list = {\n",
    "        \"train\": [],\n",
    "        \"test\": []\n",
    "    }\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        print(f\"Epoch {epoch+1}/{num_epoch}\", end=\"\")\n",
    "\n",
    "        #each epoch has a training and a validation phase\n",
    "        for phase in [\"train\", \"test\"]:\n",
    "            time_start = time.time()\n",
    "\n",
    "            model.train() if phase == \"train\" else model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0.0\n",
    "\n",
    "            for i, (images, labels) in enumerate(dataloaders[phase]):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == \"train\"):\n",
    "                    outputs = model(images)\n",
    "                    output_one_hot = torch.argmax(outputs, dim=1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training loop\n",
    "                    if phase == \"train\":\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                if phase == \"train\" and (i+1) % int(dataloader_sizes[phase]/10) == 0:\n",
    "                    print(\"-\", end=\" \")\n",
    "                        \n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                running_corrects += torch.sum(output_one_hot == labels.data)\n",
    "\n",
    "            if schedular != None and phase == \"train\":\n",
    "                    schedular.step()\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = 100 * running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            epoch_time = time.time() - time_start\n",
    "            if phase == \"train\":\n",
    "                train_time += epoch_time\n",
    "\n",
    "            print(f\"\\n{phase} Loss: {epoch_loss:.2f} Acc: {epoch_acc:.2f}%\", f\"Time_Taken: {epoch_time//60:.0f}m {epoch_time%60:.0f}s\", end=\"\")\n",
    "\n",
    "            epoch_loss_list[phase].append(float(epoch_loss))\n",
    "            epoch_acc_list[phase].append(float(epoch_acc))\n",
    "\n",
    "            # deep copy the best model\n",
    "            if phase == \"test\" and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(f\"Training Finished in {train_time//60:.0f}m {train_time%60:.0f}s\")\n",
    "    print(f\"Best Test Accuracy: {best_acc:4f}\")\n",
    "    print(model.__class__.__name__ + \"_checkpoint_best_acc_\" + str(f\"{best_acc:.4f}\") + \"_\" + \"_epoch_\" + str(num_epoch) + \"_optim_\" + optimizer.__class__.__name__ + \"_criterion_\" + criterion.__class__.__name__)\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    checkpoint = {\n",
    "        \"epoch\": num_epoch,\n",
    "        \"criterion\": criterion,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optim_state\": optimizer.state_dict(),\n",
    "        \"epoch_losses\": epoch_loss_list,\n",
    "        \"epoch_accuracies\": epoch_acc_list\n",
    "    }\n",
    "    if save_checkpoint == True:\n",
    "        file_name = model.__class__.__name__ + \"_checkpoint_best_acc_\" + str(f\"{best_acc:.4f}\") + \"_\" + \"_epoch_\" + str(num_epoch) + \"_optim_\" + optimizer.__class__.__name__ + \"_criterion_\" + criterion.__class__.__name__ + \".pth\"\n",
    "        torch.save(checkpoint, file_name)\n",
    "    \n",
    "    plot_acc_loss(epoch_loss_list, epoch_acc_list)\n",
    "\n",
    "    return model, checkpoint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
